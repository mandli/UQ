{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Spectral Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Purpose:** Construct surrogate (or Reduce Order Models) to help reduce the cost of UQ approaches.\n",
    "\n",
    "Examples:\n",
    " - Bayesian model calibration, sensitivity analysis, design and control.\n",
    " - Posterior density sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Idea:** Exploit smoothness of high-dimensional parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral Representation of Random Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some definitions:\n",
    "\n",
    "- Define a sequence of random variables\n",
    "$$\n",
    "    \\left \\{ Q_k(\\omega) \\right \\}^\\infty_{k=1}\n",
    "$$\n",
    "on the sample space $\\Omega$ and probability space $(\\Omega, \\mathcal F, P)$.\n",
    "\n",
    "- Let $\\mathbb P_k$ denote space or polynomials with argument $Q_i$ with degree $\\leq k$.\n",
    "- Let $\\hat{\\mathcal P}_k \\in \\mathbb P_k$ s.t. $P \\in \\hat{\\mathcal P}_k$ are orthogonal to $\\mathbb P_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Polynomial Expansions\n",
    "\n",
    "For a second order, finite-variance random variable $u$ can be represented as\n",
    "$$\n",
    "    u(\\omega) = u_0 \\hat{\\!P}_0 + \\sum^\\infty_{i_1 = 1} u_{i_1} \\hat{\\!P}_1(Q_{i_1}) + \\sum^\\infty_{i_1 = 1} \\sum^\\infty_{i_2 = 1} u_{i_1, i_2} \\hat{\\!P}_2(Q_{i_1}, Q_{i_2}) + \\sum^\\infty_{i_1 = 1} \\sum^\\infty_{i_2 = 1} \\sum^\\infty_{i_4 = 1} u_{i_1, i_2, i_3} \\hat{\\!P}_3(Q_{i_1}, Q_{i_2}, Q_{i_3}) + \\cdots\n",
    "$$\n",
    "Here $ \\hat{\\!P}_k(\\cdot)$ represent the interaction between variables and $u_{i_1}, u_{i_1, i_2}, \\ldots \\in \\mathbb R$.\n",
    "\n",
    "More succinctly this can be written as\n",
    "$$\n",
    "    u(Q) = \\sum^\\infty_{k=0} u_k \\Psi_k(Q_1, Q_2, Q_3, \\ldots)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we have a finite set of $\\left \\{Q_i\\right\\}^p_{k=1}$ then we have a finite sum that can represent $u(\\omega)$ up to $K$-order interactions as\n",
    "$$\n",
    "    u^K(Q) = \\sum^K_{k=0} u_k \\Psi_k(Q_1, Q_2, Q_3, \\ldots)\n",
    "$$\n",
    "where $K+1 = \\frac{(n+p)!}{n!p!}$.\n",
    "\n",
    "For example if the random variable $u$ has only second-order interactions and less we would have\n",
    "$$\n",
    "u(\\omega) = u_0 \\hat{\\!P}_0 + \\sum^\\infty_{i_1 = 1} u_{i_1} \\hat{\\!P}_1(Q_{i_1}) + \\sum^\\infty_{i_1 = 1} \\sum^\\infty_{i_2 = 1} u_{i_1, i_2} \\hat{\\!P}_2(Q_{i_1}, Q_{i_2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now consider a random process $u(t, x, \\omega)$ where now we have also added possible time and spatial dependence.  Our polynomial representation then can be written as\n",
    "$$\n",
    "    u^K(t, x, Q) = \\sum^K_{k=0} u_k(t, x) \\Psi_k(Q)\n",
    "$$\n",
    "where we note that we have separated the time and spatial dependence from the random variables.  The $\\Psi_k$ are then usually looked as a orthogonal polynomial basis for the random part of the process $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "Consider a single random variable $Q \\in C_0$ and take $\\Psi_k(Q)$ as some set of one-dimensional polynomials that are orthogonal to each other with respect to the density $\\rho_Q(q)$ and normalized so that $\\Psi_0 = 1$.  Then\n",
    "$$\n",
    "    \\mathbb E[\\psi_0(Q)] = 1\n",
    "$$\n",
    "and\n",
    "$$\\begin{aligned}\n",
    "    \\mathbb E[\\psi_i(Q) \\psi_j(Q)] &= \\int \\psi_i(Q) \\psi_j(Q) \\rho_Q(q) dq \\\\\n",
    "    &= \\langle \\psi_i(Q) \\psi_j(Q) \\rangle_\\rho \\\\\n",
    "    &= \\delta_{ij} \\gamma_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that this is analogous to projecting onto an orthogonal polynomial basis.  Compute the mean and variance of the process $u(t, x, \\omega)$ given the above rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Mean:**\n",
    "$$\\begin{aligned}\n",
    "    \\mathbb E [u^K(t, x, Q)] &= \\mathbb E \\left [ \\sum^K_{k=0} u_k(t, x) \\psi_k(Q) \\right ] \\\\\n",
    "    &= u_0(t, x) \\mathbb E[\\psi_0(Q)] + \\sum^K_{k=1} u_k(t, x) \\mathbb E[\\psi_k(Q)] \\\\\n",
    "    &= u_0(t, x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Variance:**\n",
    "$$\\begin{aligned}\n",
    "    \\text{var} [u^K(t, x, Q)] &= \\mathbb E \\left [\\left ( u^K(t, x, Q) - \\mathbb E[u^K(t, x, Q)] \\right )^2 \\right] \\\\\n",
    "    &= \\mathbb E \\left [\\left(\\sum^K_{k=0} u_k(t, x) \\psi_k(Q) - u_0(t,x) \\right )^2 \\right ] \\\\\n",
    "    &= \\mathbb E \\left [\\left(u_0(t,x) + \\sum^K_{k=1} u_k(t, x) \\psi_k(Q) - u_0(t,x) \\right )^2 \\right ] \\\\\n",
    "    &= \\mathbb E \\left [\\left(\\sum^K_{k=1} u_k(t, x) \\psi_k(Q) \\right )^2 \\right ] \\\\\n",
    "    &= \\sum^K_{k=1} u^2_k(t, x)\\gamma_k\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basis for Distributions\n",
    "\n",
    "As mentioned we want to have a polynomial basis that is orthogonal w.r.t. a density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Normal Distribution** $Q \\sim N(0, 1)$\n",
    "$$\n",
    "    \\rho_Q(q) = \\frac{1}{\\sqrt{2 \\pi}} e^{-q^2 / 2}\n",
    "$$\n",
    "defined on $\\mathbb R$, use the Hermite polynomials:\n",
    "$$\\begin{aligned}\n",
    "    &H_0(Q) = 1, & & H_1(Q) = Q, & & H_2(Q) = Q^2 - 1, \\\\\n",
    "    &H_3(Q) = Q^3 - 3 Q, & & H_4(Q) = Q^4 - 6Q^2 + 3, & & H_5(Q) = Q^5 - 10 Q^3 + 15 Q, \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Normalization constants:\n",
    "$$\n",
    "    \\gamma_i = \\int_{\\mathbb R} \\psi^2(q) \\rho_Q(q)dq = i!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Uniform Distribution** $Q \\sim \\mathcal{U}(-1, 1)$\n",
    "$$\n",
    "    \\rho_Q(q) = \\frac{1}{2}\n",
    "$$\n",
    "defined on $[-1, 1]$.\n",
    "For a uniform distribution we use the Legendre polynomials:\n",
    "$$\\begin{aligned}\n",
    "    &P_0(Q) = 1, & & P_1(Q) = Q, & & P_2(Q) = \\frac{3}{2} Q^2 - \\frac{1}{2}, \\\\\n",
    "    &P_3(Q) = \\frac{5}{2} Q^3 - \\frac{3}{2} Q, & & P_4(Q) = \\frac{35}{8} Q^4 - \\frac{15}{4} Q^2 + \\frac{3}{8}, & & P_5(Q) = \\frac{63}{8} Q^5 - \\frac{70}{8} Q^3 + \\frac{15}{8} Q, \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:** Take $u \\sim N(\\mu, \\sigma^2)$ that can be represented as\n",
    "$$\n",
    "    u = \\mu + \\sigma Q\n",
    "$$\n",
    "where $Q \\sim N(0, 1)$.  Compute the coefficients of the polynomial representation of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    \\mu = \\mathbb E \\left [ \\sum^K_{k=0} u_k(t, x) \\psi_k(Q) \\right ] = u_0(t, x) \\Rightarrow u_0 = \\mu\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\sigma^2 &= \\mathbb E \\left [\\left ( u^K(t, x, Q) - \\mathbb E[u^K(t, x, Q)] \\right )^2 \\right] \\\\\n",
    "    &= \\sum^K_{k=1} u^2_k(t, x) \\gamma_k \\\\\n",
    "    &= u^2_1(t, x) \\gamma_1 \\Rightarrow u_1 = \\sigma\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:** Take $u \\sim \\mathcal U(a, b)$ that has mean and variance\n",
    "$$\n",
    "    \\mu = \\frac{a+b}{2} \\quad \\quad \\sigma^2 = \\frac{(b - a)^2}{12}\n",
    "$$\n",
    "and can be expressed as\n",
    "$$\n",
    "    u = \\mu + \\sqrt{3} \\sigma Q\n",
    "$$\n",
    "where $Q \\sim \\mathcal{U}(-1, 1)$.  Compute the coefficients of the polynomial representation of $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly to the case of a normal distribution we have\n",
    "$$\n",
    "    u_0 = \\mu = \\frac{a + b}{2}\n",
    "$$\n",
    "and\n",
    "$$\\begin{aligned}\n",
    "    u_1 = \\sqrt{3} \\sigma\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiple Random Variables\n",
    "\n",
    "The single random variable case naturally extends to multiple random variables if we assume that the variables are assumed to be independent of each other (not necessarily the case).  This implies that the expectation of their product is the expectation of the individual variables multiplied together and motivates the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A *p-dimensional Multi-Index* is a $p$-tuple where\n",
    "$$\n",
    "    \\boldsymbol{k'} = (k_1, \\ldots, k_p) \\in \\mathbb N^p_0\n",
    "$$\n",
    "of non-negative integers with magnitude\n",
    "$$\n",
    "    |\\boldsymbol{k'}| = \\sum^p_{i=1} k_i\n",
    "$$\n",
    "and are ordered such that\n",
    "$$\n",
    "    \\boldsymbol{j'} \\leq \\boldsymbol{k'} \\iff j_i \\leq k_i \\text{  for  } i=1, \\ldots, p.\n",
    "$$\n",
    "This is a bit hard to deal with but table 10.1 provides some values for the first few multi-indices as\n",
    "\n",
    "| $k$ | $|\\boldsymbol{k'}|$ | Multi-Index | Polynomial                                 |\n",
    "|--------------------------------------------------------------------------------------|\n",
    "|0    | 0                   | (0, 0, 0)   | $\\psi_0(Q_1) \\psi_0(Q_2) \\psi_0(Q_3)$      |\n",
    "|1    | 1                   | (1, 0, 0)   | $\\psi_1(Q_1) \\psi_0(Q_2) \\psi_0(Q_3)$      |\n",
    "|2    |                     | (0, 1, 0)   | $\\psi_0(Q_1) \\psi_1(Q_2) \\psi_0(Q_3)$      |\n",
    "|3    |                     | (0, 0, 1)   | $\\psi_0(Q_1) \\psi_0(Q_2) \\psi_1(Q_3)$      |\n",
    "|4    | 2                   | (2, 0, 0)   | $\\psi_2(Q_1) \\psi_0(Q_2) \\psi_1(Q_3)$      |\n",
    "|5    |                     | (1, 1, 0)   | $\\psi_1(Q_1) \\psi_1(Q_2) \\psi_1(Q_3)$      |\n",
    "|6    |                     | (1, 0, 1)   | $\\psi_1(Q_1) \\psi_0(Q_2) \\psi_1(Q_3)$      |\n",
    "|7    |                     | (0, 2, 0)   | $\\psi_0(Q_1) \\psi_2(Q_2) \\psi_0(Q_3)$      |\n",
    "|8    |                     | (0, 1, 1)   | $\\psi_0(Q_1) \\psi_1(Q_2) \\psi_1(Q_3)$      |\n",
    "|9    |                     | (0, 0, 2)   | $\\psi_0(Q_1) \\psi_0(Q_2) \\psi_2(Q_3)$      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now define a vector of random variables \n",
    "$$\n",
    "    Q = [Q_1,\\ldots,Q_p]\n",
    "$$ \n",
    "that are mutually independent with the density\n",
    "$$\n",
    "    \\rho_Q = \\prod^p_{i=1} \\rho_{Q_p}.\n",
    "$$\n",
    "\n",
    "Let the univariate basis functions of each $Q_i$ be\n",
    "$$\n",
    "    \\left \\{ \\psi_k(Q_i) \\right \\}^K_{k=0}\n",
    "$$\n",
    "is the univariate basis functions of degree $\\leq K$ for variable $Q_i$.  We can then form the multivariate basis as\n",
    "$$\n",
    "    \\Psi_{\\boldsymbol{i'}}(Q) = \\psi_{i_1}(Q_1), \\cdots \\psi_{i_p}(Q_p)\n",
    "$$\n",
    "for $0 \\leq |\\boldsymbol{i'}| \\leq K$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The resulting basis functions therefore satisfy\n",
    "$$\\begin{aligned}\n",
    "    \\mathbb E[\\Psi_{\\boldsymbol{i'}}(Q) \\Psi_{\\boldsymbol{j'}}(Q)] &= \\int \\Psi_{\\boldsymbol{i'}}(q) \\Psi_{\\boldsymbol{j'}}(q) \\rho_Q(q) dq \\\\\n",
    "    &= \\langle \\Psi_{\\boldsymbol{i'}}, \\Psi_{\\boldsymbol{j'}} \\rangle_\\rho \\\\\n",
    "    &= \\delta_{\\boldsymbol{i'} \\boldsymbol{j'}} \\gamma_{\\boldsymbol{i'}}\n",
    "\\end{aligned}$$\n",
    "where\n",
    "$$\n",
    "    \\gamma_{\\boldsymbol{i'}} = \\mathbb{E}[\\Psi^2_{\\boldsymbol{i'}}] = \\gamma_{i_1} \\cdots \\gamma_{i_p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Turning back now to the representation of our process we have for $u(t, x, Q)$ the expansion\n",
    "$$\n",
    "    u^K(t, x, Q) = \\sum^K_{|\\boldsymbol{k'}| = 0} u_{\\boldsymbol{k'}}(t, x) \\Psi_{\\boldsymbol{k'}}(Q),\n",
    "$$\n",
    "again the projection of u(t, x, Q) onto the basis $\\Psi_{\\boldsymbol{k'}}$.  Moreover the orthogonality of the basis functions allow us to write down\n",
    "$$\n",
    "    u_k(t,x) = \\frac{1}{\\gamma_k} \\mathbb E[u(t, x, Q) \\Psi_k(Q) ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover the orthogonality of the basis functions allow us to write down\n",
    "$$\n",
    "    u_k(t,x) = \\frac{1}{\\gamma_k} \\mathbb E[u(t, x, Q) \\Psi_k(Q) ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Galerkin, Collocation, and Discrete Projection Frameworks\n",
    "\n",
    "We now turn to ways to compute the $u_k(t,x) using constraints provided by the assumptions of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finite Elements\n",
    "\n",
    "As an aside we will briefly describe the relatively similar notation and ideas from finite elements and how they will relate to the methods for computing the coefficients $u_k(t, x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the simple ODE\n",
    "$$\n",
    "    \\frac{\\text{d}^2 u}{\\text{d}x^2} = f(x) \\quad x \\in \\Omega \\quad u|_{\\partial \\Omega} = \\Gamma(x).\n",
    "$$\n",
    "\n",
    "The first thing we will do is write the equation above, a.k.a. the *strong-form* of the equation, in the *weak-form* instead.  We do this by multiplying by a test function $v$ that satisfies the boundary conditions $v|_{\\partial \\Omega} = 0$ (you can also require $u$ to do this) and integrating to find\n",
    "$$\n",
    "    \\int_{\\Omega} u''(x) v(x) dx = \\int_{\\Omega} v(x) f(x) dx.\n",
    "$$\n",
    "\n",
    "Integrating the LHS by parts leads to\n",
    "$$\\begin{aligned}\n",
    "    u'(x) v(x) |_{\\partial \\Omega} - \\int_{\\Omega} u'(x) v'(x) dx &= \\int_{\\Omega} v(x) f(x) dx. \\\\\n",
    "    - \\int_{\\Omega} u'(x) v'(x) dx &= \\int_{\\Omega} v(x) f(x) dx.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the weak-form of the equation should be true $\\forall v \\in H^1_0(\\Omega)$ such that we can restate the problem as \n",
    "$$\n",
    "    \\text{find a } u \\in H^1_0(\\Omega) \\quad \\forall v \\in H^1_0(\\Omega) \\quad \\int_{\\Omega} u'(x) v'(x) dx = \\int_{\\Omega} v(x) f(x) dx.\n",
    "$$\n",
    "This is an infinite dimensional problem and where discretization occurs.  Instead of the above problem we replace the Sobolev space with a finite dimensional space $V$ such that the problem is now to\n",
    "$$\n",
    "    \\text{find a } u \\in U \\quad \\forall v \\in V \\quad \\int_{\\Omega} u'(x) v'(x) dx = \\int_{\\Omega} v(x) f(x) dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For finite element methods we generally pick compactly support (with regards to the domain) piece-wise defined polynomials such as the hat functions\n",
    "$$\n",
    "    \\psi_i(x) = \\left \\{ \\begin{aligned}\n",
    "        &\\frac{x - x_{i-1}}{x_i - x_{i-1}} & & \\text{if } x \\in [x_{i-1}, x_i] \\\\\n",
    "        &\\frac{x_{i+1} - x}{x_{i+1} - x_{i}} & & \\text{if } x \\in [x_{i}, x_{i+1}] \\\\\n",
    "        &0 & & \\text{otherwise}\n",
    "    \\end{aligned} \\right .\n",
    "$$\n",
    "Note with this example that the basis is orthogonal at the nodes of the grid $x_i$ and have overlapping support in the intervals between nodes.  Other choices, such as the Fourier basis, lead to other methods such as spectral methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Along with the choice of basis functions is the choice of the finite dimensional function spaces $U$ and $V$ we use to solve the problem in:\n",
    "1. **Galerkin:** If the spaces $U$ and $V$ are taken to be identical then the method is called Galerkin.\n",
    "2. **?:** If the spaces are taken to be different then these are called **?**.\n",
    "3. **Collocation:** \n",
    "\n",
    "Different choices of $U$ and $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way to think of the finite dimensional problem is to think of the problem as we have defined it as a projection onto the function space $U$ and $V$.  The function $f(x)$ is being projected onto $V$ and $U$ defines the space of functions we can look at to solve the problem (the search space).  If a problem converges to the true solution then $U \\rightarrow H$ where $H$ contains the true solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Different types of perspectives\n",
    " - Minimize the residual\n",
    " - Collocation\n",
    " - Galerkin\n",
    " - Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Galerkin Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collocation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discrete Projection Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
